{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed251a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf1d05",
   "metadata": {},
   "source": [
    "These two lines import TensorFlow and the layers module from Keras, a high-level API for building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271edc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'dataset/Train',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e67e4",
   "metadata": {},
   "source": [
    "train_data: A variable that stores the dataset returned by the function.\n",
    "\n",
    "tf.keras.preprocessing.image_dataset_from_directory(): A function from TensorFlow's Keras API that loads the dataset from the directory of image files and converts it into a tf.data.Dataset object that can be used for training the machine learning model.\n",
    "\n",
    "'dataset/Train': The directory path from which to load the images for the training dataset.\n",
    "\n",
    "labels='inferred': This parameter specifies how the labels for each image should be determined. In this case, it is set to \"inferred\" so that the subdirectory names in the \"Train\" directory are used as the labels for each image.\n",
    "\n",
    "label_mode='categorical': This parameter specifies how the labels should be encoded. In this case, it is set to \"categorical\" so that the labels are one-hot encoded.\n",
    "\n",
    "batch_size=32: The number of samples in each batch of the dataset.\n",
    "\n",
    "image_size=(128, 128): The size of each image in the dataset after being resized.\n",
    "\n",
    "color_mode='grayscale': The color mode of the images. In this case, it is set to \"grayscale\" which means each image will be converted to grayscale before being used.\n",
    "\n",
    "Overall, this code loads the dataset from the \"Train\" directory, converts it into a tf.data.Dataset object, and preprocesses the data so that it is ready for use in training the OCR model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'dataset/Validation',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19517d6",
   "metadata": {},
   "source": [
    "val_data: A variable that stores the dataset returned by the function.\n",
    "\n",
    "tf.keras.preprocessing.image_dataset_from_directory(): A function from TensorFlow's Keras API that loads the dataset from the directory of image files and converts it into a tf.data.Dataset object that can be used for evaluating the machine learning model.\n",
    "\n",
    "'dataset/Validation': The directory path from which to load the images for the validation dataset.\n",
    "\n",
    "labels='inferred': This parameter specifies how the labels for each image should be determined. In this case, it is set to \"inferred\" so that the subdirectory names in the \"Validation\" directory are used as the labels for each image.\n",
    "\n",
    "label_mode='categorical': This parameter specifies how the labels should be encoded. In this case, it is set to \"categorical\" so that the labels are one-hot encoded.\n",
    "\n",
    "batch_size=32: The number of samples in each batch of the dataset.\n",
    "\n",
    "image_size=(128, 128): The size of each image in the dataset after being resized.\n",
    "\n",
    "color_mode='grayscale': The color mode of the images. In this case, it is set to \"grayscale\" which means each image will be converted to grayscale before being used.\n",
    "\n",
    "Overall, this code loads the validation dataset from the \"Validation\" directory, converts it into a tf.data.Dataset object, and preprocesses the data so that it is ready for use in evaluating the OCR model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a245a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OCR model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(128, 128, 1)),\n",
    "    layers.Conv2D(16, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(39, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3baced8",
   "metadata": {},
   "source": [
    "model: A variable that stores the OCR model architecture.\n",
    "\n",
    "tf.keras.Sequential(): A function from TensorFlow's Keras API that allows for creating a sequential model by passing a list of layers as an argument.\n",
    "\n",
    "layers.experimental.preprocessing.Rescaling(1./255, input_shape=(128, 128, 1)): This line creates a layer that rescales the pixel values of the image by dividing each pixel value by 255.0, which normalizes the pixel values to be between 0 and 1. This layer also sets the input shape of the image to be (128, 128, 1), which indicates that each input image will be a grayscale image with a height and width of 128 pixels.\n",
    "\n",
    "layers.Conv2D(16, 3, activation='relu'): This line creates a convolutional layer with 16 filters and a filter size of 3x3. The activation parameter is set to \"relu\", which means that the output of this layer will be passed through a rectified linear unit (ReLU) activation function.\n",
    "\n",
    "layers.MaxPooling2D(): This line creates a max pooling layer that reduces the spatial dimensions of the output of the previous convolutional layer by taking the maximum value within each pooling window. This helps to reduce the dimensionality of the data and extract the most important features.\n",
    "\n",
    "The next two lines (layers.Conv2D(32, 3, activation='relu') and layers.MaxPooling2D()) are similar to the previous two lines, but they create a convolutional layer with 32 filters and a max pooling layer, respectively.\n",
    "\n",
    "The next two lines (layers.Conv2D(64, 3, activation='relu') and layers.MaxPooling2D()) are again similar to the previous two lines, but they create a convolutional layer with 64 filters and a max pooling layer, respectively.\n",
    "\n",
    "layers.Flatten(): This layer flattens the output of the previous max pooling layer, which is a 2D matrix, into a 1D vector. This prepares the data to be passed through a fully connected neural network.\n",
    "\n",
    "layers.Dense(128, activation='relu'): This line creates a fully connected layer with 128 neurons and a ReLU activation function.\n",
    "\n",
    "layers.Dense(39, activation='softmax'): This line creates another fully connected layer with 39 neurons, which corresponds to the number of classes in the dataset. The activation parameter is set to \"softmax\", which normalizes the output of the layer so that each output value represents the probability that the input belongs to a particular class.\n",
    "\n",
    "Overall, this code defines the OCR model architecture using a combination of convolutional and fully connected layers, which allows the model to extract important features from the input images and make accurate predictions about the characters in the images.\n",
    "\n",
    "RESULTS==>\n",
    "Found 834036 files belonging to 39 classes.\n",
    "Found 22524 files belonging to 39 classes.\n",
    "26064/26064 [==============================] - 4986s 191ms/step - loss: 0.2349 - accuracy: 0.9244 - val_loss: 0.1869 - val_accuracy: 0.9415\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030dff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the OCR model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be350579",
   "metadata": {},
   "source": [
    "model.compile: This method configures the model for training. It takes the optimizer, loss function, and metrics as arguments.\n",
    "\n",
    "optimizer='adam': This sets the optimizer to the Adam optimizer, which is a popular gradient descent optimization algorithm.\n",
    "\n",
    "loss=tf.losses.CategoricalCrossentropy(): This sets the loss function to categorical cross-entropy. Categorical cross-entropy is commonly used in multi-class classification problems and measures the difference between the predicted and true class labels.\n",
    "\n",
    "metrics=['accuracy']: This sets the evaluation metric to accuracy. The accuracy metric measures the percentage of correctly classified samples out of all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4786ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the OCR model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=1,\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c8522",
   "metadata": {},
   "source": [
    "history = model.fit: This method trains the model using the training dataset and validates it using the validation dataset. It returns a History object that contains the training history, which can be used to analyze the model's performance during training.\n",
    "\n",
    "train_data: This is the training dataset object that was created earlier using the image_dataset_from_directory method.\n",
    "\n",
    "validation_data=val_data: This is the validation dataset object that was created earlier using the image_dataset_from_directory method.\n",
    "\n",
    "epochs=1: This sets the number of epochs or passes through the training dataset that the model will undergo during training. An epoch is a complete iteration over the entire training dataset.\n",
    "\n",
    "batch_size=32: This sets the batch size, which is the number of samples that will be propagated through the network at once during training. In this case, 32 samples will be processed at once.\n",
    "\n",
    "The model.fit method trains the model for one epoch, i.e., it goes through the entire training dataset once, and updates the model's parameters based on the error between the predicted and true labels. At the end of each epoch, the model's performance is evaluated on the validation dataset. This process is repeated for the specified number of epochs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the OCR model\n",
    "model.save('ocr_model2.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede46e3",
   "metadata": {},
   "source": [
    "This code block saves the trained OCR model to a file named ocr_model2.h5.\n",
    "\n",
    "model.save: This method saves the trained model to a file. The file format used here is the HDF5 format, which is a data model, library, and file format for storing and managing large and complex data.\n",
    "\n",
    "'ocr_model2.h5': This is the name of the file to which the model will be saved. The .h5 extension is added to indicate that the file is in the HDF5 format.\n",
    "\n",
    "By saving the model to a file, it can be reused later without the need to retrain it. This is useful when the model is required to be deployed or used for prediction on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fceb2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 444ms/step\n",
      "Predicted label: A\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the label map\n",
    "label_map = {}\n",
    "label_map[0]='#'\n",
    "label_map[1]='$'\n",
    "label_map[2]='&'\n",
    "label_map[3]='@'\n",
    "for i in range(4, 14):\n",
    "    label_map[i] = str(i - 4)\n",
    "for i, c in enumerate('ABCDEFGHIJKLMNPQRSTUVWXYZ'):\n",
    "    label_map[14+i] = c\n",
    "\n",
    "# Load the OCR model\n",
    "model = tf.keras.models.load_model('ocr_model2.h5')\n",
    "\n",
    "# Load the sample image and preprocess it\n",
    "img = cv2.imread('A.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (128, 128))\n",
    "img = np.expand_dims(img, axis=-1)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# Predict the output label for the sample image\n",
    "pred = model.predict(img)\n",
    "pred_label = np.argmax(pred)\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"Predicted label:\",label_map[pred_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01867d57",
   "metadata": {},
   "source": [
    "np.expand_dims(img, axis=-1): This function adds an extra dimension at the end of the image array, which represents the channel dimension. For grayscale images, this dimension will have a value of 1, indicating that the image has only one channel. The axis=-1 argument specifies that the new dimension should be added at the end of the array.\n",
    "\n",
    "np.expand_dims(img, axis=0): This function adds an extra dimension at the beginning of the image array, which represents the batch dimension. The batch dimension specifies how many images are being fed into the model at once. In this case, we are preprocessing a single image, so the batch dimension will have a size of 1. The axis=0 argument specifies that the new dimension should be added at the beginning of the array.\n",
    "\n",
    "By adding these dimensions to the image array, we are conforming to the input shape of the OCR model, which expects a batch of images with shape (batch_size, height, width, channels). In this case, the batch size is 1, the height and width of the image are 128 pixels each, and there is only 1 channel since the image is grayscale. The resulting shape of the preprocessed image will be (1, 128, 128, 1).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
